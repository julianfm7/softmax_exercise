{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your first neural network in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Softmax function\n",
    "\n",
    "### Motivation and defintion\n",
    "\n",
    "In many problems, a **probabilistic decision** needs to be modeled:\n",
    "- **Image Classification**: \n",
    "The output probabilities represent the likelihood of an image belonging to each possible class (e.g., a digit in MNIST).  \n",
    "- **Next-Word Prediction (NLP)**: Probabilities indicate how likely each word in the vocabulary is to appear as the next word in a sentence.\n",
    "\n",
    "The **Softmax function** is commonly used to achieve this. \n",
    "\n",
    "It takes a vector $z$ of raw scores (also known as logits) and converts them into probabilities by exponentiating each score and then normalizing them: \n",
    "$$\n",
    "\\text{softmax}(z) := \\left[\\frac{\\exp{z_1}}{c}, \\ldots, \\frac{\\exp{z_K}}{c}\\right], \\tag{SM-definition}\n",
    "$$\n",
    "being $K$ the length of $z$ (raw scores) and $c=\\sum_k \\exp{z_k}$ the **normalization factor**.\n",
    "\n",
    "This results in a discrete probability distribution with weights $w_k = \\text{softmax}(z)_k$ where \n",
    "- each value is between 0 and 1:\n",
    "$$\n",
    "0 \\leq w_k \\leq 1, \\qquad \\text{for all } k=1, \\ldots, K \\tag{Condition 1},\n",
    "$$\n",
    "- the sum of all probabilities is equal to 1:\n",
    "$$\n",
    "\\sum_k w_k = 1 \\tag{Condition 2}.\n",
    "$$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Softmax layer\n",
    "\n",
    "In this exercise, the goal is to implement a function `softmax_layer` that follows the definition in equation $\\text{(SM-definition)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here some imports that will be needed afterwards\n",
    "import numpy as np\n",
    "import ipytest \n",
    "import pytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Softmax Layer Implementation**\n",
    "\n",
    "In the cell below, fill only the lines that contain `None` to complete the implementation of the function ```softmax_layer```. \n",
    "- The input **`z`** is assumed to be a **1D NumPy array**.  \n",
    "- The output **`w`** should be another **1D NumPy array** of the same shape as `z`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Implement the softmax function\n",
    "def softmax_layer(z):\n",
    "    '''\n",
    "    Input:\n",
    "        z: a 1D numpy array of floats\n",
    "    Output:\n",
    "        w : a 1D numpy array of floats, same shape as z\n",
    "    '''\n",
    "    # THE CODE STARTS HERE\n",
    "    # exponentiate the input\n",
    "    z_exp = None\n",
    "    # compute the normalization factor\n",
    "    c = None\n",
    "    # calculate the softmax\n",
    "    w = None\n",
    "    # THE CODE ENDS HERE    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, you will find a set of tests to evaluate your solution.  \n",
    "\n",
    "**Note:** Be sure to run all previous cells first, especially the one containing your `softmax_layer` implementation.\n",
    "\n",
    "If no ```WARNING``` is printed, it means that your function has passed the tests successfully. Otherwise, you will see that some tests have not passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running test: return_None , for input z = [0 1 2] ===\n",
      "WARNING: The function should return a value.\n",
      "HINT: Check if you have filled the lines of code that calculate the softmax function.\n",
      "\u001b[31mF\u001b[0m\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_4cec39179de34ec8af6a541fec24b1b2.py::\u001b[1mtest_return_None\u001b[0m - AssertionError: The function returns None, and it should return a value.\n",
      "\u001b[31m!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 1 failures !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq --tb=no --capture=no -x\n",
    "\n",
    "# === Test no error ===\n",
    "def test_return_None(): \n",
    "    test_name = 'return_None'\n",
    "    z = np.arange(3)\n",
    "    w = softmax_layer(z)\n",
    "\n",
    "    print('=== Running test:', test_name, f', for input z = {z} ===')\n",
    "\n",
    "    if w is None:\n",
    "        general_message = \"The function should return a value.\"\n",
    "        print('WARNING:', general_message)\n",
    "        \n",
    "        hint_message = \"Check if you have filled the lines of code that calculate the softmax function.\"\n",
    "        print('HINT:', hint_message)\n",
    "        assert False, \"The function returns None, and it should return a value.\"\n",
    "\n",
    "    else:\n",
    "        print(\"The function returns a value.\")\n",
    "\n",
    "    print('=== * ===')\n",
    "\n",
    "# === Test the shape of the output ===\n",
    "cases_output_shape = [\n",
    "    (np.arange(3), (3,)),\n",
    "    (np.arange(4), (4,)),\n",
    "    (np.array([1, -2, 3, -4]), (4,))\n",
    "]\n",
    "\n",
    "@pytest.mark.parametrize(\"z, expected_shape\", cases_output_shape)\n",
    "def test_output_shape(z, expected_shape):\n",
    "    test_name = 'output_shape'\n",
    "    \n",
    "    w = softmax_layer(z)\n",
    "    \n",
    "    # check if the shape of w is correct\n",
    "    error_condition = w.shape != expected_shape\n",
    "    \n",
    "    print('=== Running test:', test_name, f', for input z = {z} ===')\n",
    "    \n",
    "    if error_condition:\n",
    "        general_message = \"The shape of the output of the softmax function is incorrect.\"\n",
    "        print('WARNING:', general_message)\n",
    "        \n",
    "        message_if_error = f\"For the input {z}, w is {w}, expected shape is {expected_shape}.\"\n",
    "        print('DETAIL:', message_if_error)\n",
    "\n",
    "        hint_message = \"Check if the output is a 1D numpy array and how the softmax function is implemented.\"\n",
    "        print('HINT:', hint_message)\n",
    "    else:\n",
    "        print(\"The shape of the output of the softmax function is correct.\")\n",
    "    \n",
    "    print('=== * ===')\n",
    "\n",
    "# === Test that the output of the softmax function is between 0 and 1 ===\n",
    "\n",
    "cases_between_zero_and_one = [\n",
    "    np.arange(4),\n",
    "    -np.arange(5),\n",
    "    np.zeros(6)\n",
    "]\n",
    "\n",
    "@pytest.mark.parametrize(\"z\", cases_between_zero_and_one)\n",
    "def test_between_zero_and_one(z):\n",
    "\n",
    "    test_name = 'between_zero_and_one'\n",
    "    \n",
    "    w = softmax_layer(z)    \n",
    "    error_condition = not np.all((w >= 0) & (w <= 1))\n",
    "\n",
    "    # check if all elements of w are between 0 and 1\n",
    "    print('=== Running test:', test_name, f', for input z = {z} ===')\n",
    "    if error_condition:        \n",
    "        general_message = \"The output of the softmax function should be between 0 and 1. Remember the (Condition 1) stated in the Definition of the Softmax.\"\n",
    "        print('WARNING:', general_message)\n",
    "        \n",
    "        message_if_error = f\"For the input {z}, w is {w}, expected all elements to be between 0 and 1.\"\n",
    "        print('DETAIL', message_if_error)\n",
    "\n",
    "        hint_message = \"Check whether the normalization and exponentiation are done correctly.\"\n",
    "        print('HINT', hint_message)\n",
    "    else:\n",
    "        print(\"The output of the softmax function is between 0 and 1.\")\n",
    "    print('=== * ===')\n",
    "\n",
    "# === Test sum of the output of the softmax function ===\n",
    "\n",
    "cases_sum_one = [\n",
    "    np.arange(3),\n",
    "    -np.arange(4),\n",
    "    np.zeros(5)\n",
    "]\n",
    "\n",
    "@pytest.mark.parametrize(\"z\", cases_sum_one)\n",
    "def test_sum_one(z):\n",
    "    test_name = 'sum_one'\n",
    "\n",
    "    w = softmax_layer(z)\n",
    "    \n",
    "    # check if the sum of w is 1\n",
    "    w_sum = np.sum(w)\n",
    "\n",
    "    error_condition = not np.isclose(w_sum, 1, atol=1e-6)\n",
    "    \n",
    "    print('=== Running test:', test_name, f', for input z = {z} ===')\n",
    "\n",
    "    if error_condition:\n",
    "        general_message = \"The sum of the output of the softmax function should be 1. Remember the (Condition 2) stated in the Definition of the Softmax.\"\n",
    "        print('WARNING:', general_message)\n",
    "        \n",
    "        message_if_error = f\"For the input {z}, sum of w is {w_sum}, expected 1.\"\n",
    "        print('DETAIL:', message_if_error)\n",
    "\n",
    "        hint_message = \"Check whether the normalization and exponentiation are done correctly.\"\n",
    "        print('HINT:', hint_message)\n",
    "    else:\n",
    "        print(\"The sum of the output of the softmax function is 1.\")\n",
    "    print('=== * ===')\n",
    "\n",
    "\n",
    "# === Test some cases where the result should be uniform ===\n",
    "cases_uniform = [\n",
    "    np.array([1, 1, 1]),\n",
    "    np.array([0, 0, 0]),\n",
    "    np.array([-1, -1, -1])\n",
    "]\n",
    "\n",
    "@pytest.mark.parametrize(\"z\", cases_uniform)\n",
    "def test_uniform(z):\n",
    "\n",
    "    test_name = 'uniform'\n",
    "\n",
    "    w = softmax_layer(z)\n",
    "    \n",
    "    # check if all elements of w are equal\n",
    "    n = len(z)\n",
    "    error_condition = not np.all(np.isclose(w, 1/n, atol=1e-6))\n",
    "\n",
    "    print('=== Running test:', test_name, f', for input z = {z} ===')\n",
    "    \n",
    "    if error_condition:\n",
    "        general_message = \"The output of the softmax function should be uniform.\"\n",
    "        print('WARNING:', general_message)\n",
    "        \n",
    "        message_if_error = f\"For the input {z}, w is {w}, expected all elements to be 1/{n}.\"\n",
    "        print('DETAIL:', message_if_error)\n",
    "    else:\n",
    "        print(\"When all elements of the input are equal, the output of the softmax function is uniform.\")\n",
    "    \n",
    "    print('=== * ===')\n",
    "\n",
    "# === Test some particular cases ===\n",
    "\n",
    "cases_particular = [\n",
    "    (np.array([1, 2, 3]), np.array([0.09003057, 0.24472847, 0.66524096])),\n",
    "    (np.array([1, -2, 3, -4]), np.array([0.11840512, 0.00589504, 0.87490203, 0.00079781])),\n",
    "    (np.array([1, 2, 3, 4, 5]), np.array([0.01165623, 0.03168492, 0.08612854, 0.23412166, 0.63640865]))\n",
    "]\n",
    "\n",
    "@pytest.mark.parametrize(\"z, expected_w\", cases_particular)\n",
    "def test_particular(z, expected_w):\n",
    "\n",
    "    test_name = 'particular'\n",
    "\n",
    "    w = softmax_layer(z)\n",
    "    \n",
    "    error_condition = not np.all(np.isclose(w, expected_w, atol=1e-6))\n",
    "\n",
    "    print('=== Running test:', test_name, f', for input z = {z} ===')\n",
    "\n",
    "    if error_condition:\n",
    "        general_message = \"The output of the softmax function is incorrect.\"\n",
    "        print('WARNING:', general_message)\n",
    "        \n",
    "        message_if_error = f\"For the input {z}, w is {w}, expected {expected_w}.\"\n",
    "        print('DETAIL:', message_if_error)\n",
    "    else:\n",
    "        print(\"The output of the softmax function is correct.\")\n",
    "    \n",
    "    print('=== * ===')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: \n",
    "def softmax_layer(z):\n",
    "    '''\n",
    "    Input:\n",
    "        z: a 1D numpy array of floats\n",
    "    Output:\n",
    "        w : a 1D numpy array of floats, same shape as z\n",
    "    '''\n",
    "    # THE CODE STARTS HERE\n",
    "    # Compute the exponential of z\n",
    "    z_exp = np.exp(z)\n",
    "    # Compute the normalization factor c\n",
    "    c = np.sum(z_exp)\n",
    "    # Compute the softmax function\n",
    "    w = z_exp /c\n",
    "    # THE CODE ENDS HERE    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical considerations\n",
    "### A small note about its implementation\n",
    "A common trick when implementing the Softmax function is to subtract the **maximum value** from the logits before applying the exponential function:\n",
    "$$\n",
    "\\tilde{z}:= [z_1 - m, \\ldots, z_K - m], \\quad m=\\max_k z_k.\n",
    "$$\n",
    "\n",
    "This helps prevent numerical instability due to large exponentiations, which can cause overflow errors. \n",
    "This operation doesn't affect the final output ($\\text{softmax}(\\tilde{z})=\\text{softmax}({z})$) but ensures the computation is more stable and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For z=[1000 2000 3000], the output of the softmax function is [inf inf inf].\n",
      "The expected result is: [0 0 1]\n",
      "This is the type of error that can happen when the input is large.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5830/1679690229.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  z_exp = np.exp(z)\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to check the numerical inestability of the softmax function for large inputs\n",
    "\n",
    "z_large = np.array([1000, 2000, 3000])\n",
    "w_large = softmax_layer(z_large)\n",
    "\n",
    "expected_w = np.array([0, 0, 1])\n",
    "\n",
    "\n",
    "print(f'For z={z_large}, the output of the softmax function is {w_large}.')\n",
    "print('The expected result is:', expected_w)\n",
    "print('This is the type of error that can happen when the input is large.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following exercise is a variation of the previous one.  \n",
    "You need to complete another implementation of `softmax_layer`, this time applying the small trick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Implement the softmax function\n",
    "def softmax_layer_bis(z):\n",
    "    '''\n",
    "    Input:\n",
    "        z: a 1D numpy array of floats\n",
    "    Output:\n",
    "        w : a 1D numpy array of floats, same shape as z\n",
    "    '''\n",
    "    # THE CODE STARTS HERE\n",
    "    m = None\n",
    "    z_tilde = None\n",
    "    z_exp = None\n",
    "    c = None\n",
    "    w = None\n",
    "    # THE CODE ENDS HERE    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For z=[1000 2000 3000], the output of the softmax function is [0. 0. 1.].\n",
      "The expected result is: [0 0 1]\n",
      "The new implementation solves the numerical instability problem!\n"
     ]
    }
   ],
   "source": [
    "# Let's check the result of the new implementation for the previous large input\n",
    "\n",
    "w_large_bis = softmax_layer_bis(z_large)\n",
    "\n",
    "print(f'For z={z_large}, the output of the softmax function is {w_large_bis}.')\n",
    "print('The expected result is:', expected_w)\n",
    "\n",
    "is_equal = np.all(np.isclose(w_large_bis, expected_w))\n",
    "if is_equal:\n",
    "    print('The new implementation solves the numerical instability problem!')\n",
    "else:\n",
    "    print('The new implementation does not solve the numerical instability problem.')\n",
    "    print('Is z_tilde calculated correctly?')\n",
    "    print('Is the normalization done correctly?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO THE BIS PROBLEM\n",
    "def softmax_layer_bis(z):\n",
    "    '''\n",
    "    Input:\n",
    "        z: a 1D numpy array of floats\n",
    "    Output:\n",
    "        w : a 1D numpy array of floats, same shape as z\n",
    "    '''\n",
    "    # THE CODE STARTS HERE\n",
    "    # Compute the maximum of z\n",
    "    m = np.max(z)\n",
    "    # Subtract the maximum from z\n",
    "    z_tilde = z - m\n",
    "    # Compute the exponential of z_tilde\n",
    "    z_exp = np.exp(z_tilde)\n",
    "    # Compute the normalization factor c\n",
    "    c = np.sum(z_exp)\n",
    "    w = z_exp /c\n",
    "    # THE CODE ENDS HERE    \n",
    "    return w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
